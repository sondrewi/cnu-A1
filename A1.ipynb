{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment A1 [35 marks]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assignment consists of 4 exercises. Each exercise may contain coding and/or discussion questions.\n",
    "- Type your **code** in the **code cells** provided below each question.\n",
    "- For **discussion** questions, use the **Markdown cells** provided below each question, indicated by ðŸ“. Double-click these cells to edit them, and run them to display your Markdown-formatted text. Please refer to the Week 1 tutorial notebook for Markdown syntax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Question 1: Numerical Linear Algebra [8 marks]\n",
    "\n",
    "**1.1** Using the method of your choice, solve the linear system $Ax = b$ with\n",
    "\n",
    "$$ A = \\begin{pmatrix}\n",
    "          1 &  1 & 0 & 1  \\\\ \n",
    "         -1 &  0 & 1 & 1  \\\\ \n",
    "          0 & -1 & 0 & -1  \\\\ \n",
    "          1 & 0 & 1 & 0 \n",
    "        \\end{pmatrix}\n",
    "        \\qquad \\text{and} \\qquad \n",
    "    b = \\begin{pmatrix}\n",
    "           5.2 \\cr 0.1 \\cr 1.9 \\cr 0\n",
    "        \\end{pmatrix},\n",
    "$$\n",
    "\n",
    "and compute the residual norm $r = \\|Ax-b\\|_2$. Display the value of $r$ in a clear and easily readable manner.\n",
    "\n",
    "**[2 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The system of solutions is [  7.1 -16.2  -7.1  14.3]\n",
      "\n",
      "Residual norm r=3.9511502286300045e-15\n"
     ]
    }
   ],
   "source": [
    "import numpy as np #import numpy\n",
    "\n",
    "def system_solver(A,b):\n",
    "    '''Define a function that will solve a system of linear equations represented \n",
    "    by a matrix A and a vector b. The function will use Gaussian elimination'''\n",
    "    def row_op(A, alpha, i, beta, j): \n",
    "        '''This function applies row operation beta*A_j + alpha*A_i to A_j,\n",
    "        the jth row of the matrix A. Changes A in place. This function and the\n",
    "        description is taken from the week04 tutorial for Computing and Numerics.\n",
    "        '''\n",
    "        A[j, :] = np.add(beta * A[j, :], alpha * A[i, :])\n",
    "        \n",
    "    def switch_nonzero(A ,p , q):\n",
    "        '''If a element [p,q] in matrix is zero, this function switches row p with one\n",
    "        below it in which the element in column q is not zero'''\n",
    "        latch0 = A[p, :].copy #create copy of the relevant row\n",
    "        latch1 = 0\n",
    "        latch2 = 0\n",
    "        for i in A[:,q]: #find first element in relevant column below A[p,q] which is not 0.\n",
    "            if i != 0 and list(A[:, q]).index(i) > p:\n",
    "                latch1 = i\n",
    "                latch2 = list(A[:, q]).index(i)\n",
    "                break \n",
    "        A[p,:]=A[latch2,:] #switch the two rows\n",
    "        A[latch2,:]=latch0\n",
    "        \n",
    "\n",
    "    def RREF(A, b):\n",
    "        '''This function constructs the augmented matrix (A|b) and applies the necessary\n",
    "        row operations using row_op() so as to reduce (A|b) to Reduced Row Echelon form. This code\n",
    "        is an adapted version of REF() function that I wrote during week04 tutorial of Coputing and Numerics'''\n",
    "        n = len(list(b))\n",
    "        C = A.copy()\n",
    "        C = np.c_[A, b] #Concactenate A and b into augmented matrix C\n",
    "        D_list=[C.copy()] #Make list of Augmented matrix after each for operation (this is fpor reference in part 1.3)\n",
    "        for i in range(n):\n",
    "            if C[i,i]==0:\n",
    "                switch_nonzero(C,i,i)\n",
    "            row_op(C, 0, 0, 1 / C[i, i], i) #Divide Row i by the element in position [i,i] to get a one on diagonal\n",
    "            D_list.append(C.copy()) #Append matrix after operation to D_list. Will be used in part 3\n",
    "            for j in range(i + 1, n): \n",
    "                row_op(C, -C[j, i], i, 1, j) #Create zeros below the one on the diagonal for the ith row.\n",
    "                D_list.append(C.copy()) #Append matrix after operation to D_list\n",
    "            D=C.copy() #The augmented matrix is now in Row echelon form. Set REF to varibale D (for reference in 1.3)\n",
    "\n",
    "        for i in reversed(range(n)): #Similar to process above, but this time working upwards in the matrix\n",
    "            for j in reversed(range(i)):\n",
    "                row_op(C, -C[j, i], i, 1, j) #Produce zeros above leading one of relevant row\n",
    "                D_list.append(C.copy()) #We again a copy of C to D_list after each operation\n",
    "        d = C[:, -1] #d is now vector containing solutions\n",
    "        C = C[:, :-1] #C is identity matrix\n",
    "        \n",
    "        return C,d,D,D_list\n",
    "    \n",
    "    return RREF(A,b)\n",
    "\n",
    "def res_norm(A,b): \n",
    "    '''Define res_norm() function which finds residual 2norm.'''\n",
    "    Ax_b=(A@system_solver(A,b)[1])-b #find vector Ax_b containing Ax-b where x is vector of solutions of linear system\n",
    "    r=np.sqrt(sum([i**2 for i in Ax_b])) #r is residual norm: square root of the sum of squares of differences Ax-b\n",
    "    return r\n",
    "\n",
    "#Define the relevant arrays A and b\n",
    "A=np.array([[1.0,1.0,0.0,1.0],\n",
    "           [-1.0,0.0,1.0,1.0], \n",
    "           [0.0,-1.0,0.0,-1.0],\n",
    "           [1.0,0.0,1.0,0.0]])\n",
    "b=np.array([5.2,0.1,1.9,0.0])\n",
    "\n",
    "#Print solutions\n",
    "print(f'The system of solutions is {system_solver(A,b)[1]}')\n",
    "print()\n",
    "print(f'Residual norm r={res_norm(A,b)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2** Repeat the same calculations for the matrix\n",
    "\n",
    "$$ A = \\begin{pmatrix}\n",
    "          a &  1 & 0 & 1  \\\\ \n",
    "         -1 &  0 & 1 & 1  \\\\ \n",
    "          0 & -1 & 0 & -1  \\\\ \n",
    "          1 & 0 & 1 & 0 \n",
    "        \\end{pmatrix}\n",
    "        \\qquad \\text{with} \\qquad a \\in \\{10^{-8}, 10^{-10}, 10^{-12}\\}. \n",
    "$$\n",
    "\n",
    "Display the value of $r$ for each value of $a$, and avoid repeating (copy+pasting) code.\n",
    "\n",
    "**[3 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For a=1e-08 the system of solutions is [ 7.09999984e+08 -1.42000002e+09 -7.10000009e+08  1.42000002e+09]\n",
      "For a=1e-08 we have r=34.83209261112543\n",
      "\n",
      "For a=1e-10 the system of solutions is [ 7.09998674e+10 -1.41999977e+11 -7.09999883e+10  1.41999977e+11]\n",
      "For a=1e-10 we have r=170920.25204869633\n",
      "\n",
      "For a=1e-12 the system of solutions is [ 7.09958094e+12 -1.41992087e+13 -7.10047098e+12  1.41992087e+13]\n",
      "For a=1e-12 we have r=1226075882.9361339\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a_list=[10**(-8), 10**(-10), 10**(-12)] #Make a list containing the 'a's\n",
    "\n",
    "for a in a_list: #Iterate over relevant elements of set a to produce relevant residual norms\n",
    "    A[0,0]=a #change a for each iteration\n",
    "    print(f'For a={a} the system of solutions is {system_solver(A,b)[1]}') #print the system of solutions\n",
    "    print(f'For a={a} we have r={res_norm(A,b)}') #print the residual norm\n",
    "    print() #add some spacing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3** Summarise and explain your observations in a discussion of no more than $250$ words.\n",
    "\n",
    "**[3 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Reference for task 1.3:') \n",
    "'''Here I attempt to display some of the errors python does in doing row operations when a=10**(-12)'''\n",
    "\n",
    "A[0,0]=10**(-12) #Set a \n",
    "\n",
    "print('We now show how errors ocurr in the lower right corner of the Augmented Matrix C after certain Row operations')\n",
    "\n",
    "String_list=['C after Row op 4: R2*10^(-12)','C[2:,3:] after Row op 5: R3+R2 (add 1*R2 to R3)', \n",
    "             'C[2:,3:] after Row op 6: R4+10^(12)R2', 'C[2:,3:] after Row op 7: 10^(12)R3',\n",
    "            'C[2:,3:] after Row op 8: R4-2*R3', 'C[2:,3:] after Row op 9: -1.000055730852182*R4',\n",
    "            'C[2:,3:] after Row op 10: R3-1.000088900582341*R4'] #Make list of strings that will clarify what row operation I am demonstrating\n",
    "\n",
    "def error_analysis(s, n): \n",
    "    '''Function which prints effect of given row operation in lower right corner of augmented matrix and more accurate\n",
    "    Python representation of those numbers. In puts are a string from the above list and an integer n referring to  \n",
    "    matrix after given row operation stored in D '''\n",
    "    print()\n",
    "    np.set_printoptions() #Reset number of decimals numpy displays\n",
    "    print(s)\n",
    "    if n==5: #If n is 5 we display entire matrix\n",
    "        for i in system_solver(A,b)[3][5]:\n",
    "            print(i)\n",
    "        \n",
    "    else: #if n not 5 only display lower right corner of matrix\n",
    "        for i in system_solver(A,b)[3][n][2:,3:]:\n",
    "            print(i)\n",
    "    print()\n",
    "    #Set number of decimal places numpy shows in machine representation of number\n",
    "    np.set_printoptions(formatter={'float': lambda x: \"{0:0.25f}\".format(x)}) \n",
    "    \n",
    "    #Now display how python stores relevant numbers in part of matrix just displayed\n",
    "    if n==5: #if n is 5 show numbers in C[1:,3:]\n",
    "        print('Accurate to 25 d.p, Python stores C[1:,3:] as:')\n",
    "        for i in system_solver(A,b)[3][5][1:,3:]:\n",
    "            for j in i:\n",
    "                print(np.array([j]))\n",
    "        \n",
    "    else: #If n not 5 only show python's representation of numbers in bottom corner\n",
    "        print('Accurate to 25 d.p. Python stores these as:')\n",
    "        for i in system_solver(A,b)[3][n][2:,3:]:\n",
    "            for j in i:\n",
    "                print(np.array([j]))\n",
    "    print(20*'-')\n",
    "\n",
    "    \n",
    "for h in range(6): #Make error analysis show its results for row operations 4 to 10\n",
    "    error_analysis(String_list[h], h+5) \n",
    "    \n",
    "    \n",
    "#Now demonstrate how python stores some numbers\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.55f}\".format(x)}) #Set numpy to display 55 decimal places\n",
    "print(f'How python stores 0.1: {str(np.array([0.1]))[1:-1]}')\n",
    "print(f'How python stores 1+10^(-12): {str(np.array([1+10**(-12)]))[1:-1]}')\n",
    "print()\n",
    "print(f'Python floats are only guaranteed to be accurate to 15 sig figs: \\n0.7500000000000006 == 0.7500000000000005 -->  {0.7500000000000006 == 0.7500000000000005}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“ ***Discussion for question 1.3***\n",
    "\n",
    "(# words without $\\LaTeX$ expressions: 246)\n",
    "\n",
    "Errors arise in solutions because many floating point values are approximated by the binary representation inherent to computers (doc.python.org). As shown above, the value python stores for 0.1 is 0.1000000000000000055511151231257827021181583404541015625 to 55 d.p. Floats are only guaranteed to be accurate to 15 significant figures (docs.python.org). (See last example above).\n",
    "\n",
    "When multiplying small integers by very large ones, large errors arise because decimals are not exact. Errors accumulate as more Row operations are done. Errors are larger for smaller a because more erroneous numbers (from Python's representation) get magnified.\n",
    "Consider case when $a=10^{-12}$. Using same method as in code and working by hand on paper we get REF of C:\n",
    "\n",
    "$\\begin{pmatrix}\n",
    "1 & 10^{12} & 0 & 10^{12} & 5.2*10^{12} \\\\\n",
    "0 & 1 & 10^{-12} & 1+10^{-12}& 5.2+10^{-13}\\\\\n",
    "0 & 0 & 1 & 1 & 7.1*10^{12}+10^{-1}\\\\\n",
    "0 & 0 & 0 & 1 & 1.42*10^{13}+10^{-1}\n",
    "\\end{pmatrix}$\n",
    "\n",
    "The lower right corner is different from REF created by Python in task 1.2 (see C[2:, 3:] after row op 9).\n",
    "\n",
    "Noticeable problems arise during 5th row operation on the original augmented matrix. At C[2,3], the result is not precisely $10^{-12}$, but $1.000088900582341e-12$ (see above). This is because Python represents the number $1+10^{-12}$ as $1.0000000000010000889005823$ to 25d.p. After 6th row op the result C[2,3] is $1.0001220703125000$ (to 16 d.p.). 7th op yields $1.0000889005823410$ in C[2,3].\n",
    "8th op yields $-1.000055730852182$ in C[2,3] and $-14200000000000.098$ in C[3,4]. A big error arises after op 9 yields $14199208666000.83$ in C[3,4]. This value is unchanged for the REF python yields and is very different from that in the real REF (by several millions). \n",
    "\n",
    "Errors are magnified as Python iterates up to create zeros above leading 1s. These errors get expressed in residual norms.\n",
    " \n",
    "Sources:\n",
    "15. Floating Point Arithmetic: Issues and Limitations - Python Documentation\n",
    "URL:https://docs.python.org/3/tutorial/floatingpoint.html#:~:text=Representation%20error%20refers%20to%20the,exact%20decimal%20number%20you%20expect\n",
    "Last updated: Feb 17, 2021\n",
    "Retrieved: Feb 20, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Question 2: Sums [10 marks]\n",
    "\n",
    "Consider the sum\n",
    "\n",
    "$$\n",
    "S_N = \\sum_{n=1}^N \\frac{2n+1}{n^2(n+1)^2}.\n",
    "$$\n",
    "\n",
    "**2.1** Write a function `sum_S()` which takes 2 input arguments, a positive integer `N` and a string `direction`, and computes the sum $S_N$ **iteratively** (i.e. using a loop):\n",
    "- in the direction of increasing $n$ if `direction` is `'up'`,\n",
    "- in the direction of decreasing $n$ if `direction` is `'down'`.\n",
    "\n",
    "For instance, to calculate $S_{10}$ in the direction of decreasing $n$, you would call your function using `sum_S(10, 'down')`.\n",
    "\n",
    "**[3 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_S(N, direction): \n",
    "    '''See that each term in the sum above can also be expressed (1/n^2)-(1/((n+1)^2).\n",
    "    This function calculates the sum above by exploiting this fact. For each n, (1/n^2) is only calculated \n",
    "    once so as to reduce number of calculations. Calculating the sum this way is much quicker and yields \n",
    "    the same result as I will explain in section 3.3 (I also checked this).'''\n",
    "    the_sum = 0 #define the sum\n",
    "    \n",
    "    if direction == 'up': #conditional if 'up'\n",
    "        \n",
    "        first_term = 1 #for n=1 (1/n^2)=1\n",
    "        for n in range(1, N + 1):\n",
    "            n_plus_1 = ((1 / (n + 1))**2) #define 1/((n+1)^2 for relevant n\n",
    "            the_sum += first_term - n_plus_1 #compute (1/n^2)-(1/((n+1)^2) and add to the_sum\n",
    "            first_term = n_plus_1 #set first_term to 1/((n+1)^2 to prepare for next iteration\n",
    "        return the_sum, first_term #return the_sum as well as last first term (will be useful for 2.2)\n",
    "    \n",
    "    else: #if direction not 'up'. We use reverse process of 'up' method above\n",
    "        \n",
    "        second_term = ((1 / (N + 1))**2) #1/((n+1)^2 for biggest n\n",
    "        for n in reversed(range(1, N + 1)):\n",
    "            n_term = ((1 / n)**2) #define 1/((n+1)^2 for relevant n\n",
    "            the_sum += n_term - second_term #add (1/n^2)-(1/((n+1)^2) to the_sum\n",
    "            second_term = n_term #set second_term to 1/n^2 to prepare for next iteration\n",
    "            \n",
    "        return the_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2** The sum $S_N$ has the closed-form expression $S_N = 1-\\frac{1}{(N+1)^2}$. We assume that we can compute $S_N$ using this expression without significant loss of precision, i.e. that we can use this expression to obtain a \"ground truth\" value for $S_N$.\n",
    "\n",
    "Using your function `sum_S()`, compute $S_N$ iteratively in both directions, for 10 different values of $N$, linearly spaced, between $10^3$ and $10^6$ (inclusive).\n",
    "\n",
    "For each value of $N$, compare the results in each direction with the closed-form expression. Present your results graphically, in a clear and understandable manner.\n",
    "\n",
    "**[4 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_list = [] #List of sums for 'up' method\n",
    "down_list = [] #List of sums for 'down' method\n",
    "closed_list = [] #List of sums using close-form expression\n",
    "up_err = [] #error of 'up' method\n",
    "down_err = [] #error of 'down' method\n",
    "\n",
    "\n",
    "N_list = [int(N) for N in np.linspace(10**3, (10**6), num=10)] #define the N values as specified\n",
    "\n",
    "for N in N_list: #Loop over the Ns to produce sums for 'up', 'down', and closed form methods\n",
    "    \n",
    "    up_sum, first_term = sum_S(N, 'up') #Get the up_sum and term 1//(N+1)^2 embodied by first_term. \n",
    "    #Immediately extracting first_term reduces number of necessary calculations of square of very large numbers\n",
    "    up_list.append(up_sum)#Append 'up' sum\n",
    "    down_list.append(sum_S(N, 'down')) #Append 'down' sum\n",
    "    closed_list.append(1 - first_term) #this follows from the definition of the close-form expression\n",
    "    up_err.append((abs(up_list[-1] - closed_list[-1]))) #We compare methods by examining absolute error. Append absolute error of 'up' method\n",
    "    down_err.append((abs(down_list[-1] - closed_list[-1]))) #Append error of down method\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Here I make a small table to display errors of 'up' and down methods\n",
    "print('N-value' + 3 * ' ' + '|Error sum_S(N,\"up\") to 16 d.p. ' + \n",
    "      '|Error sum_S(N,\"down\") to 16 d.p.')\n",
    "for i in range(10):\n",
    "    print(str(N_list[i]) + (10 - len(str(N_list[i]))) * ' ' + '|' +\n",
    "          str(up_err[i]) + (31 - len(str(up_err[i]))) * ' ' + '|' + str(down_err[i]))\n",
    "\n",
    "fig,ax=plt.subplots(1,2,figsize=(15, 6)) #construct to subplots next to each other\n",
    "#In first plot, show sums of each method on log-scale to better demonstrate differences\n",
    "#In order to more easily construct a proper log-scale, we subtract 1 from all sum values and then reconfigure axis tick labels\n",
    "ax[0].plot(N_list, [i-1 for i in up_list], 'go-',markersize=10, label='Values using sum_S(N,\"up\")', alpha=0.5) #Set opacity to 0.5 to be able to differentiate overlapping data-points\n",
    "ax[0].plot(N_list, [i-1 for i in down_list], 'ro-',markersize=10, label='Values using sum_S(N,\"down\")', alpha=0.4)\n",
    "ax[0].plot(N_list, [i-1 for i in closed_list],'b^', label=r'$S_N$')\n",
    "ax[0].set_ylim(-10**(-5.8), 10**(-11))\n",
    "ax[0].set_yscale('symlog',linthreshy=10**(-11)) #Define a threshhold within which scale is not log to better see differences between methods\n",
    "ax[0].set_xticks(N_list)\n",
    "ax[0].set_xticklabels(N_list,rotation=45)\n",
    "ax[0].set_yticks([-10**(-i) for i in reversed(range(6,12))]+[0,10**(-11)]) #Define y-scale ticks\n",
    "ax[0].set_yticklabels([f'1-1e-{i}' for i in reversed(range(6,12))]+[1,'1+1e-11']) #Reconfigure y-ticks to compensate the earlier subtraction of 1\n",
    "ax[0].set_ylabel(r'$Sum$', fontsize=14)\n",
    "ax[0].set_xlabel('Value of N', fontsize=14)\n",
    "ax[0].legend(loc='lower right', fontsize=14)\n",
    "ax[0].set_title(r'Graph 1: Result of Methods Against N (log scale y-axis)')\n",
    "\n",
    "#On second subplot demonstrate absolute error of 'up' method against 'down method'\n",
    "ax[1].plot(N_list, up_err, 'g*-', label='Absolute Error sum_S(N,\"up\")')\n",
    "ax[1].plot(N_list, down_err, 'r*-', label='Absolute Error sum_S(N,\"down\")')\n",
    "ax[1].set_ylim(-10**(-16), 10**(-11)) #Set limits of y-scale\n",
    "ax[1].set_yscale('symlog', linthreshy=10**(-15.7)) #Again set threshold within whcih graph is not on log-scale so as to demonstrate error of 'down' method\n",
    "ax[1].set_xticks(N_list)\n",
    "ax[1].set_xticklabels(N_list,rotation=45)\n",
    "ax[1].set_yticks([]) #Reset y_ticks\n",
    "ax[1].set_yticks([0]+[10**(-i) for i in reversed(range(11,17))]) #make y axis increasing\n",
    "ax[1].set_yticklabels([0]+[10**(-i) for i in reversed(range(11,17))])\n",
    "ax[1].set_ylabel(r'Absolute Error from $S_N$', fontsize=14)\n",
    "ax[1].set_xlabel('Value of N', fontsize=14)\n",
    "ax[1].legend(loc='center right', fontsize=14)\n",
    "ax[1].set_title(r'Graph 2: Absolute Error Relative to $S_N$ against N (log scale y-axis)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3** Describe and explain your findings in no more that $250$ words. Which direction of summation provides the more accurate result? Why?\n",
    "\n",
    "**[3 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference examples for 2.3\n",
      "Sum of small numbers similar order of magnitude: 10^(-18)+3*10^(-18) = 0.0000000000000000040000000000000002861697\n",
      "----\n",
      "Sum of numbers of very different order of magnitude: 0.1+3*10^(-18)  = 0.1000000000000000055511151231257827021182\n",
      "We see that this is precisely the same as what python stores for 0.1 = 0.1000000000000000055511151231257827021182\n"
     ]
    }
   ],
   "source": [
    "print('Reference examples for 2.3')\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.40f}\".format(x)}) #Set number of decimal places numpy displays\n",
    "#show some calculations in numpy:\n",
    "print(f'Sum of small numbers similar order of magnitude: 10^(-18)+3*10^(-18) = {str(np.array([10**(-18)+3*10**(-18)]))[1:-1]}')\n",
    "print('----')\n",
    "print(f'Sum of numbers of very different order of magnitude: 0.1+3*10^(-18)  = {str(np.array([0.1+3*10**(-18)]))[1:-1]}')\n",
    "print(f'We see that this is precisely the same as what python stores for 0.1 = {str(np.array([0.1]))[1:-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“ ***Discussion for question 2.3***\n",
    "\n",
    "(# words: 250)\n",
    "\n",
    "The 'down' direction provides the more accurate result (graphs and table in 2.2). Floating point values stored by python are accurate only to 15 or 16 significant figures (due to storing in binary). After each addition, Python rounds the resulting sum to a number it can represent correct to 15 or 16 significant figures (Goldberg, 1991). Notice that with 'down' method, Python adds numbers to the_sum in ascending order. Thus, the very small numbers do not get 'rounded away' as they do in the 'up' method, where numbers are added in descending order. With 'down' method, the sum of the very small numbers grows to such a size that when it gets added to the bigger ones, it is not rounded away.\n",
    "\n",
    "We illustrate this with an example that is shown above. Making numpy use 40 d.p we see that when doing a sum such as $10^{-18}+3*10^{-18}$, python stores the result as $0.0000000000000000040000000000000002861697$, which is accurate to 16 significant figures. \n",
    "However, when python does the sum $0.1+3*10^{-18}$ it stores the result as $0.1000000000000000055511151231257827021182$, which is precisely the same as how python stores 0.1.\n",
    "\n",
    "Obviously, for small enough N, the 'up' method (with which we start by adding 'large' numbers to the_sum) will be relatively accurate since the small numbers do not get so small as to get 'rounded away' once they get added to the_sum.\n",
    "\n",
    "Additionally, as can be seen from graph 2, small rounding errors can also ocurr for 'down' method since the sum to which numbers are added is growing.\n",
    "\n",
    "Source: \n",
    "'What Every Computer Scientist Should Know About Floating-Point Arithmetic'\\\\\\\n",
    "Computing Surveys \\\\\\\n",
    "Goldberg, David \\\\\\\n",
    "March, 1991 \\\\\\\n",
    "URL: https://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Question 3: Numerical Integration [10 marks]\n",
    "\n",
    "For integer $k \\neq 0$ consider the integral \n",
    "\n",
    "$$\n",
    "I(k) = \\int_0^{2\\pi}  x^4 \\cos{k x} \\ dx = \\frac{32\\pi^3}{k^2} - \\frac{48\\pi}{k^4} \\ .\n",
    "$$\n",
    "\n",
    "**3.1** Write a function `simpson_I()` which takes 2 input arguments, `k` and `N`, and implements Simpson's rule to compute and return an approximation of $I(k)$, partitioning the interval $[0, 2\\pi]$ into $N$ sub-intervals of equal width.\n",
    "\n",
    "**[2 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''I employ Simpson's composite rule and assume that N should refer to the number of subintervals, each of which \n",
    "will have a quadrature calculated on it.'''\n",
    "\n",
    "def f(x,k): #Define the relevant function\n",
    "    return (x**4)*np.cos(k*x)\n",
    "\n",
    "def quadrature(f, xk, wk, a, b, k): #Here I use the quadrature function and docstring from w06 tutorial \n",
    "    '''\n",
    "    Approximates the integral of f over [a, b],\n",
    "    using the quadrature rule with weights wk\n",
    "    and nodes xk.\n",
    "    \n",
    "    Input:\n",
    "    f (function): function to integrate (as a Python function object)\n",
    "    xk (Numpy array): vector containing all nodes\n",
    "    wk (Numpy array): vector containing all weights\n",
    "    a (float): left boundary of the interval\n",
    "    b (float): right boundary of the interval\n",
    "    \n",
    "    Returns:\n",
    "    I_approx (float): the approximate value of the integral\n",
    "        of f over [a, b], using the quadrature rule.\n",
    "    '''\n",
    "    # Define the shifted and scaled nodes\n",
    "    yk = ((b-a)/2)*(xk+1)+a\n",
    "    \n",
    "    # Compute the weighted sum\n",
    "    I_approx = ((b-a)/2)*(sum(wk*f(yk, k)))\n",
    "    \n",
    "    return I_approx\n",
    "\n",
    "def simpson_I(k,N):\n",
    "    '''This function should return the approximation for the integral using composite Simpson's rule \n",
    "    for a given value of k and when N subintervals are used (the quadrature splitting each subinterval into 2 parts)'''\n",
    "    \n",
    "    xk=np.array([-1.,0.,1.]) #Define xk and wk for Simpson's as calculated in w06\n",
    "    wk=np.array([1/3, 4/3, 1/3])\n",
    "    \n",
    "    bounds=np.linspace(0.0,2*np.pi,N+1) #Create an array of N+1 nodes representing bounds for subintervals\n",
    "    the_sum=0 #Create variable of value zero named the_sum\n",
    "    \n",
    "    for i in range(N): #Run quadrature for each subsection contained in bounds and add to the_sum\n",
    "        the_sum+=quadrature(f,xk,wk,bounds[i],bounds[i+1], k) \n",
    "\n",
    "    return the_sum #return the_sum which should now contain our approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2** For $k = 1$, and for $\\varepsilon \\in \\{10^{-n} \\ |\\  n \\in \\mathbb{N}, 3 \\leqslant n \\leqslant 8\\}$, determine the number $N_{\\text{min}}$ of partitions needed to get the value of the integral $I(1)$ correctly to within $\\varepsilon$. \n",
    "\n",
    "**[2 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsi_set=[10**(-i) for i in range(3,9)] #Define set of epsilons\n",
    "\n",
    "def I_exact(k): #Define function which calculates exact integral for given k\n",
    "    return ((32*(np.pi**3)/(k**2))-(48*np.pi/(k**4)))\n",
    "\n",
    "def N_min_func(k):\n",
    "    '''This function finds the N_mins values for each epsilon given a certain k. \n",
    "    By formula for composite quadrature rules E=alpha*h^r in w06, this function assumes that error \n",
    "    is monotonically decreasing for decreasing h (increasing N)'''\n",
    "    \n",
    "    N_mins = [0] #Make N_min list with one element 0\n",
    "    I_ex=I_exact(k) #Define the exact value of integral for the given k\n",
    "    M_list=[0, 500, 100, 20, 5, 1] #Make list of 'jumps' in N value that function will do to check error at different places\n",
    "    \n",
    "    for e in epsi_set: #Iterate over set of epsilons\n",
    "        M=N_mins[-1] #First, set M to the last element in N_min since N for smaller epsilon is assumed to be larger\n",
    "        for j in range(1,6): #Iterate over the elements in M_list\n",
    "            \n",
    "            M-=M_list[j-1] #Take M_minus previous element in M_list to make sure no values are 'skipped'\n",
    "            while abs(simpson_I(k, M) - I_ex) > e: #While error is larger, keep adding relevant element in M_list to M\n",
    "                M+=M_list[j]\n",
    "            #When abs(simpson_I(k, M) - I_ex) <= e, function will jump back M_list[j-1] places and start making smaller jumps \n",
    "            #to check for condition abs(simpson_I(k, M) - I_ex) > e. The last jumps are of size 1 and this is how function find N_min\n",
    "        \n",
    "        N_mins.append(M) #Append last value of M to N_mins since this will be smallest M for which abs(simpson_I(k, M) - I_ex) <= e\n",
    "        \n",
    "    return N_mins[1:] #Return N_mins without the starting 0\n",
    "\n",
    "print(f'N_min for k=1 (each successive element corresponds to tenfold decrease epsilon): \\n{N_min_func(1)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3** Repeat your calculations from **3.2** for $k \\in \\{2^{n}\\ |\\ n \\in \\mathbb{N}, n \\leqslant 6\\}$. \n",
    "\n",
    "**[2 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=[N_min_func(1)] #Initiate list to store N_mins for each value of k\n",
    "\n",
    "ks=[1,2,4,8,16,32,64] #list of k's\n",
    "\n",
    "for k in ks[1:]: #Iterate over k-values\n",
    "    mins=N_min_func(k) #Define mins so that N_min_func(k) does not have to be calculated twice\n",
    "    \n",
    "    print(f'for k = {k} we have N_min = {mins} for e in epsi_set, respectively') #display results\n",
    "    \n",
    "    A.append(mins) #Append N_mins for each k to A\n",
    "    \n",
    "A=np.array(A) #make A an np.array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3** Present your results graphically by plotting \n",
    "\n",
    "(a) the number of terms $N_{\\text{min}}$ against $\\varepsilon$ for fixed $k$, \n",
    "\n",
    "(b) the number of terms $N_{\\text{min}}$ against $k$ for fixed $\\varepsilon$.\n",
    "\n",
    "You should format the plots so that the data presentation is clear and easy to understand.\n",
    "\n",
    "**[2 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1,2,figsize=(20, 10)) #Create two subplots\n",
    "plot_types=['g^-', 'b^-', 'r^-', 'k^-', 'c^-', 'm^-', 'y^-'] #list of plot types that I will use in plot (color, marker, and line)\n",
    "\n",
    "#In first subplot we plot reversed list of epsilons against the relevant N_mins as stored in rows of A\n",
    "#In other words we plot epsilon against the minimum N such that |approximation - exact integral|<epsilon for constant k\n",
    "\n",
    "for i in range(7): #Loop over range 0-6 to plot reversed rows in A (in log10) against reversed epsi_set (in log10). \n",
    "    ax[0].plot(np.log10(epsi_set[::-1]),np.log10(A[i][::-1]), plot_types[i], markersize=5, label=r'$\\log_{10}(N_{min})$ vs $\\log_{10}({\\varepsilon})$ ' + f'for k={ks[i]}') \n",
    "    #note that I have reversed epsi_set so that log10(epsilon) is growing along the x-axis\n",
    "    \n",
    "#Specify Title, axis labels, and location + size of legend for Graph 1:\n",
    "ax[0].set_ylabel(r'$\\log_{10}(N_{min})$', fontsize=16)\n",
    "ax[0].set_xlabel(r'$\\log_{10}({\\varepsilon})$', fontsize=17)\n",
    "ax[0].legend(loc='upper right', fontsize=13)\n",
    "ax[0].set_title(r'Graph 1: $\\log_{10}(N_{min})$ against $\\log_{10}({\\varepsilon})$ for fixed $k$', fontsize=18)\n",
    "\n",
    "#In second subplot we plot ks against relevant N_mins as stored in columns of B\n",
    "#In other words we plot k against the minimum N such that |approximation - exact integral|<epsilon for constant epsilon\n",
    "\n",
    "for i in range(6): #Loop over range 0-6 to plot columns in A (in log2 and starting at last column) against ks.\n",
    "    ax[1].plot(np.log2(ks), np.log2(A[:,5-i]), plot_types[i], markersize=5, label=r'$\\log_{10}(N_{min})$ vs $\\log_{2}({k})$ for $\\varepsilon$=' + f'1e-{8-i}')\n",
    "\n",
    "#Specify Title, axis labels, and location + size of legend for Graph 2:\n",
    "ax[1].set_ylabel(r'$\\log_{2}(N_{min})$', fontsize=16)\n",
    "ax[1].set_xlabel(r'$\\log_{2}({k})$', fontsize=14)\n",
    "ax[1].legend(loc='upper left', fontsize=13)\n",
    "ax[1].set_title(r'Graph 2: $\\log_{10}(N_{min})$ against $\\log_{2}({k})$ for fixed $\\varepsilon$', fontsize=18)\n",
    "\n",
    "plt.show() #show graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.4** Discuss, with reference to your plot from 3.3, your results. Your answer should be no more than $250$ words.\n",
    "\n",
    "**[2 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('References for task 3.3:')\n",
    "np.set_printoptions() #reset number of decimal places numpy displays\n",
    "\n",
    "#Compute and print coefficients for lines of best fit for data points in Graph 1 (keeping k constant). \n",
    "#We do this using the inbuilt numpy function polyfit, which returns the coefficients for line of best fit (highest degree first)\n",
    "#Documentation for polyfit that I consulted is here: https://numpy.org/doc/stable/reference/generated/numpy.polyfit.html\n",
    "print(r'Coefficients of epsilon in line of best fit for data points in Graph 1:')\n",
    "print()\n",
    "coeff1_sum=0 #create as sum of coefficients used for taking average later\n",
    "\n",
    "#Use polyfit with log10(error) (error stored in reversed epsi_set) as independent variable \n",
    "#and log10(N_min) (stored in reversed rows of A) as dependent variable \n",
    "#Do this for each row of A:\n",
    "for i in range(7):\n",
    "    coefficient=np.polyfit(np.log10(epsi_set[::-1]), np.log10(A[i][::-1]),  1)[0] \n",
    "    print(r'Coefficient of epsilon ' + f'for k={ks[i]}: ' + str(coefficient))\n",
    "    coeff1_sum+=coefficient #add to the sum of coefficients\n",
    "print(20*'-')\n",
    "print()\n",
    "\n",
    "#Compute and print coefficients for lines of best fit for data points in Graph 2 (keeping epsilon constant):\n",
    "print(r'Coefficients of k in line of best fit for data points in Graph 2:')\n",
    "print()\n",
    "coeff2_sum=0 #create as sum of coefficients used for taking average later\n",
    "\n",
    "#Use polyfit with log2(k) (k stored in ks) as independent variable \n",
    "#and log2(N_min) (N_min stored in columns of A) as dependent variable \n",
    "#Do this for each column of A:\n",
    "for i in range(6):\n",
    "    coefficient = np.polyfit(np.log2(ks), np.log2(np.array(A)[:,i]), 1)[0]\n",
    "    print(r'Coefficient of k ' + f'for epsilon={epsi_set[i]}: ' + str(coefficient))\n",
    "    coeff2_sum+=coefficient #add to the sum of coefficients\n",
    "print(20*'-')\n",
    "print()\n",
    "\n",
    "#Print average of coefficients for each of the above:\n",
    "print(f'Average of coefficients Graph 1: {coeff1_sum/7}')\n",
    "print(f'Average of coefficients Graph 2: {coeff2_sum/6}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“ ***Discussion for question 3.4***\n",
    "\n",
    "(# words without $\\LaTeX$ expressions: 248)\n",
    "\n",
    "For constant k, there is an approximately linear relationship between $\\log_{10}{\\varepsilon}$ and $\\log_{10}{N_{min}}$ (graph 1). This is expressed in formula describing the error of composite quadrature rules (tutorial w06)\n",
    "$$\n",
    "\\varepsilon = \\alpha h^r = \\alpha (\\frac{1}{N})^r,\n",
    "$$\n",
    "which, when taking $\\log_{10}$ on both sides, rearranges to\n",
    "$$\n",
    "\\frac{-\\log_{10}{\\varepsilon} + \\log_{10}{\\alpha}}{r} = \\log_{10}({N}).\n",
    "$$\n",
    "\n",
    "$$\n",
    "$$\n",
    "\n",
    "Since each $N_{min}$ is smallest N such that approximation error < $\\varepsilon$, consecutive data-points in Graph 1 do $not$ correspond to a decrease in N such that error of approximation increases $precisely$ tenfold. However, by assuming it does, we can apply above formula if we remember that results will have some small error.\n",
    "\n",
    "\n",
    "Using numpy's polyfit module, for constant k, the coefficient of the line of best fit between $\\log_{10}{\\varepsilon}$ and $\\log_{10}{N_{min}}$ is always close to -0.249 (see above). Average of coefficients gives $\\approx -0.2488641$. Treating $\\log{\\alpha}$ as a constant and using the formula:\n",
    "$$$$\n",
    "$$\n",
    "\\log_{2}({N_2}) - \\log_{2}({N_1}) = \\frac{-\\log_{10}{\\varepsilon_1} - 1 + \\log_{10}{\\alpha}}{r} - \\frac{-\\log_{10}{\\varepsilon_1} + \\log_{10}{\\alpha}}{r} \\approx -0.2488641\n",
    "$$\n",
    "$$$$\n",
    "$$\n",
    "\\Rightarrow -0.2488641 \\approx \\frac{-1}{r},\n",
    "$$\n",
    "and thus\n",
    "$$\n",
    "r \\approx 4.018\n",
    "$$\n",
    "as approximate (to 4 d.p) rate of convergence of composite Simpson rule that we can extrapolate from data.\n",
    "\n",
    "Graph 2 shows an approximate linear relationship between $\\log_{2}{k}$ and $\\log_{2}{N_{min}}$. Using polyfit (and keeping in mind the ineherent error as mentioned above), we get a coefficient always close to 0.502. Average $\\approx 0.502073129$. In the above equation, changes to k must get expressed in $\\alpha$.\n",
    "Thus, treating $\\varepsilon$ as constant, and considering change of 1 in $\\log_{2}{k}$\n",
    "$$$$\n",
    "$$\n",
    "\\log_{2}({N_2}) - \\log_{2}({N_1}) = \\frac{-\\log_{2}{\\varepsilon} + \\log_{2}{\\alpha_1} + \\Delta\\log_{2}{\\alpha}}{r} - \\frac{-\\log_{2}{\\varepsilon} + \\log_{2}{\\alpha_1}}{r} \\approx 0.502073129\n",
    "$$\n",
    "$$$$\n",
    "$$\n",
    "\\Rightarrow \\frac{\\Delta\\log_{2}{\\alpha}}{r} \\approx 0.502073129\n",
    "$$\n",
    "and thus\n",
    "$$\n",
    "\\Delta\\log_{2}{\\alpha} \\approx 2.017\n",
    "$$ to 4 d.p. Written differently,\n",
    "$$\n",
    "\\log_{2}{\\alpha} \\approx 2.017\\log_{2}{k} + C\n",
    "$$\n",
    "$$$$\n",
    "$$\n",
    "\\Rightarrow \\alpha \\approx 2^C*k^{2.017}\n",
    "$$\n",
    "$$$$\n",
    "The reason that larger $N_{min}$s result for larger $k$s for same $\\varepsilon$ is that changes in x cause 'quicker' changes in $x^4\\cos(kx)$. Thus, the interpolating polynomials of degree two within Simpson's rule become less accurate (relative to actual graph). Since k changes at constant relative rate, the graph compresses at constant relative rate, leading to constant relative rate change in $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Question 4: Numerical Derivatives [7 marks]\n",
    "\n",
    "Derivatives can be approximated by finite differences in several ways, for instance\n",
    "\n",
    "\\begin{align*}\n",
    "        \\frac{df}{dx} & \\approx \\frac{f(x+h) - f(x)}{h} \\\\\n",
    "        \\frac{df}{dx} & \\approx \\frac{f(x) - f(x-h)}{h}  \\\\\n",
    "        \\frac{df}{dx} & \\approx \\frac{f(x+h) - f(x-h)}{2h} \\ . \n",
    "\\end{align*}\n",
    "\n",
    "Assuming $f$ to be differentiable, in the limit $h \\to 0$, all three expressions are equivalent and exact, but for finite $h$ there are differences. Further discrepancies also arise when using finite precision arithmetic.\n",
    "\n",
    "**4.1**\n",
    "Estimate numerically the derivative of $f(x) = \\cos(x)$ at $x = 1$ using the three expressions given above and different step sizes $h$. Use at least 50 logarithmically spaced values $h \\in [10^{-16}, 10^{-1}]$.\n",
    "\n",
    "**[2 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h=np.logspace(-16,-1,num=100) #create array of 100 logarithmically spaced values in specified range\n",
    "\n",
    "est1_list=(np.cos(1+h)-np.cos(1))/h #create an array of values containing forward derivative approximations for each h\n",
    "est2_list=(np.cos(1)-np.cos(1-h))/h #create an array of values containing backward derivative approximations for each h\n",
    "est3_list=(np.cos(1+h)-np.cos(1-h))/(2*h) #create an array of values containing centered derivative approximations for each h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.2**\n",
    "Display the absolute difference between the numerical results and the\n",
    "exact value of the derivative, against $h$ in a doubly logarithmic plot. \n",
    "You should format the plot so that the data presentation is clear and easy to understand.\n",
    "\n",
    "**[2 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note that (d/dx)cos(x))-sin(x)\n",
    "\n",
    "err1=abs(est1_list+np.sin(1)) #Compute absolute error for forward approximation\n",
    "err2=abs(est2_list+np.sin(1)) #Compute absolute error for backward approximation\n",
    "err3=abs(est3_list+np.sin(1)) #Compute absolute error for centered approximation\n",
    "\n",
    "fig,ax=plt.subplots(1,1,figsize=(20, 10)) #Create one (sub)plot\n",
    "#Plot the h-values against the absolute errors for each method\n",
    "ax.plot(h,err1,'g^-',markersize=5, label=r'$\\frac{df}{dx} \\approx \\frac{f(x+h)-f(x)}{h}$ error against h')\n",
    "ax.plot(h,err2,'b^-',markersize=5, label=r'$\\frac{df}{dx} \\approx \\frac{f(x)-f(x-h)}{h}$  error against h')\n",
    "ax.plot(h,err3,'r^-',markersize=5, label=r'$\\frac{df}{dx} \\approx \\frac{f(x+h)-f(x-h)}{2h}$  error against h')\n",
    "ax.set_yscale('log',basey=10) #Set log-log scale with y-scale in base 10\n",
    "ax.set_xscale('log')\n",
    "ax.set_xticks([10**(-i) for i in range(0,17)])\n",
    "ax.set_yticks([10**(-i) for i in range(1,12,2)])\n",
    "ax.set_xticklabels([10**(-i) for i in range(0,17)],rotation=45, fontsize=14)\n",
    "ax.set_yticklabels([10**(-i) for i in range(1,12,2)], fontsize=14)\n",
    "ax.set_ylabel('Error of Approximation', fontsize=18)\n",
    "ax.set_xlabel(r'$h$', fontsize=18)\n",
    "ax.set_title(r'Absolute error of approximations against h', fontsize=28)\n",
    "ax.legend(loc='lower left', fontsize=22)\n",
    "ax.minorticks_off() #Turn off annoying small ticks on y-axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.3**\n",
    "Describe and interpret your results in no more than 250 words.\n",
    "\n",
    "*Hint: run the code below.*\n",
    "\n",
    "**[3 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 1e-15\n",
    "print(1 + h - 1)\n",
    "print((1 + h - 1)/h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“ ***Discussion for question 4.3***\n",
    "\n",
    "(# words without $\\LaTeX$ expressions: 246)\n",
    "\n",
    "For $h < 10^{-8}$ the computed approximations display similar accuracy. Absolute errors of the shown magnitudes arise for $h < 10^{-8}$ because Python floats are only guaranteed accurate 15 significant figures (see task 1.3). Python uses its (binary) representation of numbers to compute sums (or differences) and then rounds the result to a number it can represent. For any $h = 10^{-x}$, imprecise numbers get amplified by $h = 10^{x}$, as numerators are divided by $h$ and $2h$. As h increases, less imprecise digits get amplified.\n",
    "\n",
    "Forward ($D_1(x)$) and backward ($D_{-1}(x)$) approximations are most accurate for $h \\approx 10^{-8}$ since they are both first order accurate (as can be dervied from tutorial 7):\n",
    "\\begin{equation}\n",
    "  D_1(x) - F' \\left( x \\right)\n",
    "  = \\frac{\\Delta x}{2}  F'' \\left( x \\right) + \\frac{\\Delta x^2}{6} F'''\\left(x\\right) + \\dots = O(\\Delta x)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "  D_{-1}(x) - F' \\left( x \\right)\n",
    "  = \\frac{-\\Delta x}{2}  F'' \\left( x \\right) + \\frac{\\Delta x^2}{6} F'''\\left(x\\right) + \\dots = O(\\Delta x)\n",
    "\\end{equation}\n",
    "\n",
    "For $h \\approx 10^{-8}$, rounding errors only magnify imprecise figures from Python's decimal representation by $\\approx 10^{8}$ and the forward and backward approximation formulae are accurate to $\\approx 10^{-8}$. As $h > 10^{-8}$, forward and backward approximations become less accurate at a (log-log) linear rate due to their error expressions. Although their error seemingly coincide for $h > 10^{-7}$, there are still small differences in their (written out) error expressions.\n",
    "\n",
    "$$\n",
    "$$\n",
    "\n",
    "The central difference approximation is the average of the other methods. For $h < 10^{-8}$, it suffers from error for the same reasons as the others. However, for $10^{-8} < h < 10^{-5}$ the central method is more accurate due to its second order accuracy (derived from formulae above):\n",
    "\n",
    "\\begin{equation}\n",
    "  D_C(x) - F' \\left( x \\right)\n",
    "  = \\frac{\\Delta x^2}{6} F'''\\left(x\\right) + \\frac{\\Delta x^4}{120} F'''''\\left(x\\right) + \\dots = O(\\Delta x^2)\n",
    "\\end{equation}\n",
    "\n",
    "For $10^{-8} < h < 10^{-5}$, the central diff. method's error is very close to zero and imprecise numbers from Python's decimal representation are decreasingly amplified. For $h > 10^{-4}$, however, the error increases log-log linearly.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
